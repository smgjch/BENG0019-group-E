{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diabetic patients readmission rates preditction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install learn2learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy import optimize\n",
    "from sklearn import datasets as skdataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from learn2learn.algorithms.maml import MAML\n",
    "from learn2learn.data import TaskDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project introduction\n",
    "\n",
    "- Overview: <br>\n",
    "This project is focusing on developing a predictive model to ascertain the likelihood of readmission for diabetes patients.\n",
    "<br>\n",
    "\n",
    "- Target:<br>\n",
    "The main goal of this project is developing a powerful machine learning model which can predict the readmission rate of patient "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading\n",
    "The following cells are used to load training and testing data for our prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"Dataset/diabetic_data_training.csv\")\n",
    "test_data = pd.read_csv(\"Dataset/diabetic_data_test.csv\")\n",
    "mapping_info = pd.read_csv(\"Dataset/IDS_mapping.csv\", header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to handle different data types for plotting\n",
    "# def plot_column(ax, column, df):\n",
    "#     if df[column].dtype == 'object':\n",
    "#         # Check if binary\n",
    "#         if df[column].nunique() == 2:\n",
    "#             # Binary data visualization\n",
    "#             df[column].value_counts().plot(kind='bar', ax=ax)\n",
    "#         else:\n",
    "#             # Categorical data visualization\n",
    "#             df[column].value_counts().plot(kind='pie', autopct='%1.1f%%', ax=ax)\n",
    "#     elif df[column].dtype == 'int64' or df[column].dtype == 'float64':\n",
    "#         # Numeric data visualization\n",
    "#         df[column].plot(kind='hist', bins=20, ax=ax)\n",
    "#     else:\n",
    "#         ax.text(0.5, 0.5, f\"Unhandled data type for column: {column}\", \n",
    "#                 fontsize=12, ha='center')\n",
    "#     ax.set_title(column)\n",
    "\n",
    "# # Creating a 4x4 subplot layout\n",
    "# fig, axes = plt.subplots(nrows=13, ncols=4, figsize=(20, 65))\n",
    "# fig.tight_layout(pad=5.0)\n",
    "\n",
    "# # Iterate through each column and plot\n",
    "# for i, col in enumerate(train_data.columns):\n",
    "#     # Adjust this line to select different subsets of columns  \n",
    "#     plot_column(axes[i//4, i%4], col,train_data)\n",
    "\n",
    "\n",
    "# plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing\n",
    "The following cells are used to preprocess the training and testing data. There are two main goals in our preprocessing data section of the code\n",
    "- Change the string type data in our dataset to integer type data \n",
    "- Apply some applicable method to full up the missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # this part just make a dictionary to mapping the id between data and ID mapping table\n",
    "# mapping_dicts = [{} for i in range(3)]\n",
    "# mapping_title = []\n",
    "# title = -1\n",
    "# for data in mapping_info.itertuples():\n",
    "#     if(pd.isna(data[1])):\n",
    "#         continue\n",
    "#     if(not data[1].isdigit()):\n",
    "#         title += 1\n",
    "#         mapping_title.append(data[1])\n",
    "#     else:\n",
    "#         if(pd.isna(data[2])):\n",
    "#             mapping_dicts[title]['NULL'] = data[1]\n",
    "#             continue\n",
    "#         mapping_dicts[title][data[2]] = data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this part is used to change all string type data to integer type\n",
    "# for the missing value, we will skip and process it at next step\n",
    "df = train_data.copy()\n",
    "\n",
    "\n",
    "df_test = test_data.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# mask = df.isna().sum(axis=1) <= 3\n",
    "# filtered_df = df[mask]\n",
    "# print(filtered_df.any(axis=1).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-Hot Encoding For race:\n",
    "cons: One-hot encoding can lead to a significant increase in the dataset's dimensionality (a problem known as the \"curse of dimensionality\"), especially if the categorical feature has many unique values. This can increase the computational cost and may require more data to achieve good performance.\n",
    "Dems Redct Would be apply, so it doesn't matter\n",
    "pros: Map to a fix number implies an ordinal relationship between the categories which may not exist, but is ideal for non-ordinal categorical data. It's suitable for many machine learning models, especially those that assume no ordinal relationship between categories\n",
    "\n",
    "\n",
    "1. random forest, remove ?\n",
    "2. randomly assign ? to a class by disstribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General Missing value\n",
    "1. multiple imputation To be decide when training if less than 1h 5 epoch\n",
    "2. mean\n",
    "3. fullly remove\n",
    "4. wrong -> fix ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding for age:\n",
    "1. Asumming normal distribution, map to a random age in the range\n",
    "2. Map to mean age in the range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Race Process\n",
    "1. Remove missing since race is proved to be a significant impact to medical result.\n",
    "2. One-hot encode race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def race_filter(df):\n",
    "    \"\"\"\n",
    "    Remove missing value from race\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.dataframe): series containing a colum of the feature matrix.\n",
    "\n",
    "    Return:\n",
    "    Filtered dataframe free from ? for race\n",
    "    \"\"\"\n",
    "    race_mask = (df['race'] != \"?\")\n",
    "    df = df[race_mask]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = race_filter(df)\n",
    "df_encoded = pd.get_dummies(df, columns=[\"race\"], prefix=\"race\",dtype=int)\n",
    "\n",
    "df_test = race_filter(df_test)\n",
    "df_test_encoded = pd.get_dummies(df_test, columns=[\"race\"], prefix=\"race\",dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gender Process\n",
    "1. Remove Unknown/Invalid and missing\n",
    "2. One hot encode Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gender_filter(df):\n",
    "    \"\"\"\n",
    "    Remove missing value from race\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.dataframe): series containing a colum of the feature matrix.\n",
    "\n",
    "    Return:\n",
    "    Filtered dataframe free from ? for race\n",
    "    \"\"\"\n",
    "    gender_mask = (df['gender'] != \"Unknown/Invalid\")\n",
    "    df = df[gender_mask]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded = gender_filter(df_encoded)\n",
    "df_test_encoded = gender_filter(df_test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_mapping = {'Male':0,'Female':1}\n",
    "df_encoded['gender'] = df_encoded['gender'].map(gender_mapping)\n",
    "df_test_encoded['gender'] = df_test_encoded['gender'].map(gender_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encounter_id and patient_nbr Process \n",
    "1. n/a\n",
    "2. Drop since it provide no info to the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded.drop(columns=['encounter_id','patient_nbr'], inplace=True)\n",
    "df_test_encoded.drop(columns=['encounter_id','patient_nbr'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Age Process\n",
    "1. n/a\n",
    "2. Map age from range to mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded['age'] = (df['age'].str.extract(r'(\\d+)-(\\d+)')[0].astype(int)+df['age'].str.extract(r'(\\d+)-(\\d+)')[1].astype(int))//2\n",
    "df_test_encoded['age'] = (df_test['age'].str.extract(r'(\\d+)-(\\d+)')[0].astype(int)+df_test['age'].str.extract(r'(\\d+)-(\\d+)')[1].astype(int))//2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weight Process\n",
    "1. Drop Weight for too many missing values and no information to predict.\n",
    "2. n/a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded.drop(columns=['weight'], inplace=True)\n",
    "df_test_encoded.drop(columns=['weight'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "admission_type_id\n",
    "discharge_disposition_id\n",
    "admission_source_id\n",
    "To be merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Payer Code and Medical Specialty Process\n",
    "1. Both droped since too many missing\n",
    "2. Payer Code not relevant as id, Medical Specialty too many categories and no info to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded.drop(columns=['payer_code'], inplace=True)\n",
    "df_encoded.drop(columns=['medical_specialty'], inplace=True)\n",
    "df_test_encoded.drop(columns=['payer_code'], inplace=True)\n",
    "df_test_encoded.drop(columns=['medical_specialty'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pc = df['payer_code'].astype('category')\n",
    "# unique_categories_pc = df_pc.cat.categories\n",
    "# medical_specialty_to_num = {medical_specialty: i for i, medical_specialty in enumerate(unique_categories_pc)}\n",
    "# max_binary_length = len(bin(len(unique_categories_pc) - 1)) - 2\n",
    "# payer_code_to_binary = {medical_specialty: bin(num)[2:].zfill(max_binary_length) for medical_specialty, num in medical_specialty_to_num.items()}\n",
    "\n",
    "# df_encoded['payer_code'] = df_encoded['payer_code'].map(payer_code_to_binary)\n",
    "\n",
    "# df_test_encoded['payer_code'] = df_test_encoded['payer_code'].map(payer_code_to_binary)\n",
    "# df_ms = df['medical_specialty'].astype('category')\n",
    "# unique_categories_ms = df_ms.cat.categories\n",
    "# medical_specialty_to_num = {medical_specialty: i for i, medical_specialty in enumerate(unique_categories_ms)}\n",
    "# max_binary_length = len(bin(len(unique_categories_ms) - 1)) - 2\n",
    "# medical_specialty_to_binary = {medical_specialty: bin(num)[2:].zfill(max_binary_length) for medical_specialty, num in medical_specialty_to_num.items()}\n",
    "\n",
    "# df_encoded['medical_specialty'] = df_encoded['medical_specialty'].map(medical_specialty_to_binary)\n",
    "\n",
    "# df_test_encoded['medical_specialty'] = df_test_encoded['medical_specialty'].map(medical_specialty_to_binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "num_lab_procedures\n",
    "num_procedures\n",
    "num_medications\n",
    "number_outpatient\n",
    "number_emergency\n",
    "number_inpatient\n",
    "No need to be encode since no missing and integer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "diag_1,diag_2 and diag_3 Process:\n",
    "1. Filte diag_1, diag_2 and diag_3 by number_diagnoses. If the amount of diagnoses in diag_1, diag_2 and diag_3 doesn't match the number_diagnoses, then remove.\n",
    "2. Layer encoding diags according to the ICD-9 Code Category, drop original 3 diag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the encoding function\n",
    "# def encode_element(element):\n",
    "#     encoding_result = [0] * 20  # Default encoding\n",
    "\n",
    "#     if pd.notna(element):\n",
    "#         if element[0].isdigit():\n",
    "#             element = float(element)\n",
    "#             # Encode based on numeric range\n",
    "#             if 1 <= element <= 139:\n",
    "#                 encoding_result[0] = 1\n",
    "#             elif 140 <= element <= 239:\n",
    "#                 encoding_result[1] = 1\n",
    "#             elif 240 <= element <= 279:\n",
    "#                 encoding_result[2] = 1\n",
    "#             elif 280 <= element <= 289:\n",
    "#                 encoding_result[3] = 1\n",
    "#             elif 290 <= element <= 319:\n",
    "#                 encoding_result[4] = 1\n",
    "#             elif 320 <= element <= 389:\n",
    "#                 encoding_result[5] = 1\n",
    "#             elif 390 <= element <= 459:\n",
    "#                 encoding_result[6] = 1\n",
    "#             elif 460 <= element <= 519:\n",
    "#                 encoding_result[7] = 1\n",
    "#             elif 520 <= element <= 579:\n",
    "#                 encoding_result[8] = 1\n",
    "#             elif 580 <= element <= 629:\n",
    "#                 encoding_result[9] = 1\n",
    "#             elif 630 <= element <= 679:\n",
    "#                 encoding_result[10] = 1\n",
    "#             elif 680 <= element <= 709:\n",
    "#                 encoding_result[11] = 1\n",
    "#             elif 710 <= element <= 739:\n",
    "#                 encoding_result[12] = 1\n",
    "#             elif 740 <= element <= 759:\n",
    "#                 encoding_result[13] = 1\n",
    "#             elif 760 <= element <= 779:\n",
    "#                 encoding_result[14] = 1\n",
    "#             elif 780 <= element <= 799:\n",
    "#                 encoding_result[15] = 1\n",
    "#             elif 800 <= element <= 999:\n",
    "#                 encoding_result[16] = 1\n",
    "#             # Add more conditions for other ranges if needed\n",
    "#         elif element[0].isalpha():\n",
    "#             # Encode based on string prefix\n",
    "#             if element.startswith('E'):\n",
    "#                 encoding_result[17] = 1\n",
    "#             elif element.startswith('V'):\n",
    "#                 encoding_result[18] = 1\n",
    "#             elif element.startswith('M'):\n",
    "#                 encoding_result[19] = 1\n",
    "#             # Add more conditions for other prefixes if needed\n",
    "\n",
    "#     return encoding_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = df[\"diag_1\"].loc[0]\n",
    "# a = encode_element(a)\n",
    "# a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diag_missing_value_filter(df):\n",
    "\n",
    "    df['number_diagnoses'] = pd.to_numeric(df['number_diagnoses'], errors='coerce')\n",
    "    mask = ((df['number_diagnoses'] <= 3) & (\n",
    "            (3 - df[['diag_1', 'diag_2', 'diag_3']].apply(lambda x: x == \"?\").sum(axis=1)) >= df['number_diagnoses'])) | (\n",
    "    (df['number_diagnoses'] > 3) & (df[['diag_1', 'diag_2', 'diag_3']].apply(lambda x: x == \"?\").sum(axis=1)) == 0) \n",
    "    df_filtered = df[mask].copy()\n",
    "\n",
    "    return df_filtered\n",
    "\n",
    "def diag_layer_encoding(df):\n",
    "\n",
    "    # Define the encoding function\n",
    "    def encode_element(element):\n",
    "        encoding_result = [0] * 20  # Default encoding\n",
    "\n",
    "        if pd.notna(element):\n",
    "            if element[0].isdigit():\n",
    "                element = float(element)\n",
    "                # Encode based on numeric range\n",
    "                if 1 <= element <= 139:\n",
    "                    encoding_result[0] = 1\n",
    "                elif 140 <= element <= 239:\n",
    "                    encoding_result[1] = 1\n",
    "                elif 240 <= element <= 279:\n",
    "                    encoding_result[2] = 1\n",
    "                elif 280 <= element <= 289:\n",
    "                    encoding_result[3] = 1\n",
    "                elif 290 <= element <= 319:\n",
    "                    encoding_result[4] = 1\n",
    "                elif 320 <= element <= 389:\n",
    "                    encoding_result[5] = 1\n",
    "                elif 390 <= element <= 459:\n",
    "                    encoding_result[6] = 1\n",
    "                elif 460 <= element <= 519:\n",
    "                    encoding_result[7] = 1\n",
    "                elif 520 <= element <= 579:\n",
    "                    encoding_result[8] = 1\n",
    "                elif 580 <= element <= 629:\n",
    "                    encoding_result[9] = 1\n",
    "                elif 630 <= element <= 679:\n",
    "                    encoding_result[10] = 1\n",
    "                elif 680 <= element <= 709:\n",
    "                    encoding_result[11] = 1\n",
    "                elif 710 <= element <= 739:\n",
    "                    encoding_result[12] = 1\n",
    "                elif 740 <= element <= 759:\n",
    "                    encoding_result[13] = 1\n",
    "                elif 760 <= element <= 779:\n",
    "                    encoding_result[14] = 1\n",
    "                elif 780 <= element <= 799:\n",
    "                    encoding_result[15] = 1\n",
    "                elif 800 <= element <= 999:\n",
    "                    encoding_result[16] = 1\n",
    "                # Add more conditions for other ranges if needed\n",
    "            elif element[0].isalpha():\n",
    "                # Encode based on string prefix\n",
    "                if element.startswith('E'):\n",
    "                    encoding_result[17] = 1\n",
    "                elif element.startswith('V'):\n",
    "                    encoding_result[18] = 1\n",
    "                elif element.startswith('M'):\n",
    "                    encoding_result[19] = 1\n",
    "                # Add more conditions for other prefixes if needed\n",
    "\n",
    "        return encoding_result\n",
    "\n",
    "    for i in range(1,4):\n",
    "        encoded_columns = df[\"diag_\"+str(i)].apply(encode_element)\n",
    "\n",
    "    # Create new columns with \"diag_1\" as a prefix\n",
    "        for j in range(20):\n",
    "            new_column_name = \"diag_\" + str(i)+\"_\"+ str(j+1)\n",
    "            df[new_column_name] = encoded_columns.apply(lambda x: x[j])\n",
    "\n",
    "    return df\n",
    "# def diag_layer_encoding(element):\n",
    "    if isinstance(element, (int,float)):\n",
    "        # Encode based on numeric range\n",
    "        if 1 <= element <= 139:\n",
    "            return [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "        elif 140 <= element <= 239:\n",
    "            return [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "        elif 240 <= element <= 279:\n",
    "            return [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "        elif 280 <= element <= 289:\n",
    "            return [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "        elif 290 <= element <= 319:\n",
    "            return [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "        elif 320 <= element <= 389:\n",
    "            return [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "        elif 390 <= element <= 459:\n",
    "            return [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "        elif 460 <= element <= 519:\n",
    "            return [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "        elif 520 <= element <= 579:\n",
    "            return [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "        elif 580 <= element <= 629:\n",
    "            return [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "        elif 630 <= element <= 679:\n",
    "            return [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "        elif 680 <= element <= 709:\n",
    "            return [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "        elif 710 <= element <= 739:\n",
    "            return [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "        elif 740 <= element <= 759:\n",
    "            return [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
    "        elif 760 <= element <= 779:\n",
    "            return [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
    "        elif 780 <= element <= 799:\n",
    "            return [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
    "        elif 800 <= element <= 999:\n",
    "            return [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
    "        # Add more conditions for other ranges if needed\n",
    "\n",
    "    elif isinstance(element, str):\n",
    "        # Encode based on string prefix\n",
    "        if element.startswith('E'):\n",
    "            return [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
    "        elif element.startswith('V'):\n",
    "            return [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
    "        elif element.startswith('M'):\n",
    "            return [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    "        # Add more conditions for other prefixes if needed\n",
    "\n",
    "    # Default case (not a number or string with known prefix)\n",
    "    return [0] * 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded = diag_missing_value_filter(df_encoded)\n",
    "df_encoded = diag_layer_encoding(df_encoded)\n",
    "df_test_encoded = diag_missing_value_filter(df_test_encoded)\n",
    "df_test_encoded = diag_layer_encoding(df_test_encoded)\n",
    "\n",
    "for i in range(1,4):\n",
    "        df_encoded.drop(columns=[f'diag_{i}'], inplace=True)\n",
    "        df_test_encoded.drop(columns=[f'diag_{i}'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "number_diagnoses pass since no missing and integer\n",
    "max_glu_serum and A1Cresult Process:\n",
    "1.n/a\n",
    "2.index map to 0-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_glu_serum_mapping = {'>200': 2, '>300': 3, 'normal': 1}\n",
    "A1Cresult_mapping = {'>8':3,'>7':2,'normal':1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded['max_glu_serum'] = df_encoded['max_glu_serum'].map(max_glu_serum_mapping).fillna(0)\n",
    "df_test_encoded['max_glu_serum'] = df_test_encoded['max_glu_serum'].map(max_glu_serum_mapping).fillna(0)\n",
    "\n",
    "df_encoded[\"A1Cresult\"] = df_encoded['A1Cresult'].map(A1Cresult_mapping).fillna(0)\n",
    "df_test_encoded[\"A1Cresult\"] = df_test_encoded['A1Cresult'].map(A1Cresult_mapping).fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "metformin\n",
    "repaglinide\n",
    "nateglinide\n",
    "chlorpropamide\n",
    "glimepiride\n",
    "acetohexamide\n",
    "glipizide\n",
    "glyburide\n",
    "tolbutamide\n",
    "pioglitazone\n",
    "rosiglitazone\n",
    "acarbose\n",
    "miglitol\n",
    "troglitazone\n",
    "tolazamide\n",
    "examide\n",
    "citoglipton\n",
    "insulin\n",
    "glyburide-metformin\n",
    "glipizide-metformin\n",
    "glimepiride-pioglitazone\n",
    "metformin-rosiglitazone\n",
    "metformin-pioglitazone\n",
    "Process:\n",
    "1. drop 'examide', 'citoglipton','glimepiride-pioglitazone' and 'metformin-rosiglitazone' since single value observed\n",
    "2. index map to 0-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded.drop(columns=['examide', 'citoglipton','glimepiride-pioglitazone','metformin-rosiglitazone'],axis=1,inplace=True)\n",
    "df_test_encoded.drop(columns=['examide', 'citoglipton','glimepiride-pioglitazone','metformin-rosiglitazone'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "medics = ['metformin','repaglinide', 'nateglinide', 'chlorpropamide', 'glimepiride',\n",
    "       'acetohexamide', 'glipizide', 'glyburide', 'tolbutamide',\n",
    "       'pioglitazone', 'rosiglitazone', 'acarbose', 'miglitol', 'troglitazone',\n",
    "       'tolazamide', 'insulin', 'glyburide-metformin', 'glipizide-metformin',\n",
    "        'metformin-pioglitazone']\n",
    "for i in medics:\n",
    "    df_encoded.loc[df_encoded[i] == 'Up', [i]] = 3  \n",
    "    df_encoded.loc[df_encoded[i] == 'Down', [i]] = 1 \n",
    "    df_encoded.loc[df_encoded[i] == 'Steady', [i]] = 2 \n",
    "    df_encoded.loc[df_encoded[i] == 'No', [i]] = 0\n",
    "    df_test_encoded.loc[df_test_encoded[i] == 'Up', [i]] = 3  \n",
    "    df_test_encoded.loc[df_test_encoded[i] == 'Down', [i]] = 1 \n",
    "    df_test_encoded.loc[df_test_encoded[i] == 'Steady', [i]] = 2 \n",
    "    df_test_encoded.loc[df_test_encoded[i] == 'No', [i]] = 0 \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "change and diabetesMed Process\n",
    "1. n/a\n",
    "2. binary map to 1/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_mapping = {'No':0,'Ch':1}\n",
    "diabetesMed_mapping = {'No':0,'Yes':1}\n",
    "\n",
    "df_encoded['change'] = df_encoded['change'].map(change_mapping)\n",
    "df_encoded['diabetesMed'] = df_encoded['diabetesMed'].map(diabetesMed_mapping)\n",
    "\n",
    "df_test_encoded['change'] = df_test_encoded['change'].map(change_mapping)\n",
    "df_test_encoded['diabetesMed'] = df_test_encoded['diabetesMed'].map(diabetesMed_mapping)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "readmitted Process\n",
    "1.n/a\n",
    "2. index mapping, No as 0, >30 as 1 and <30 as most significant as 2 and drop readmitted for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "readmitted_mapping = {'NO':0,'<30':2,'>30':1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_encoded['readmitted'] = df_encoded['readmitted'].map(readmitted_mapping)\n",
    "df_test_encoded['readmitted'] = df_test_encoded['readmitted'].map(readmitted_mapping)\n",
    "\n",
    "y_test_readmitted = df_encoded[\"readmitted\"]\n",
    "y_readmitted = df_test_encoded[\"readmitted\"]\n",
    "\n",
    "df_encoded.drop(columns=[\"readmitted\"],axis=1,inplace=True)\n",
    "df_test_encoded.drop(columns=[\"readmitted\"],axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization age, num_lab_procedures and num_medications to keep the same order of magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "features_to_scale = [\"age\",\"num_lab_procedures\",\"num_medications\"]\n",
    "df_encoded[features_to_scale] = scaler.fit_transform(df_encoded[features_to_scale])\n",
    "df_test_encoded[features_to_scale] = scaler.fit_transform(df_test_encoded[features_to_scale])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encode payer_code, medical_specialty\n",
    "1. Find correlation internally with other feature in group of non-missing value\n",
    "2. Use identified feature predict payer-code, medical_specialty\n",
    "3. Prediction algorithm to be decide, could be KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this part will reduce the dimension our training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dems Redct\n",
    "1. PCA/PPCA\n",
    "2. LDA/QDA\n",
    "3. following to T-SNE\n",
    "3. Autoencoders\n",
    "4. Unsupervised Algorithmn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-12 01:02:01.149417: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-12 01:02:01.149474: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-12 01:02:01.150691: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-12 01:02:01.156240: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-12 01:02:02.200782: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of features in your dataset\n",
    "n_features = len(df_encoded.columns)  # Change this to the actual number of features\n",
    "\n",
    "# Define the encoder\n",
    "input_layer = Input(shape=(n_features,))\n",
    "encoder = Dense(64, activation='relu')(input_layer)\n",
    "encoder = Dense(32, activation='relu')(encoder)\n",
    "\n",
    "# Define the bottleneck\n",
    "bottleneck = Dense(10, activation='relu')(encoder)  # This is the latent space representation\n",
    "\n",
    "# Define the decoder (mirror the encoder)\n",
    "decoder = Dense(32, activation='relu')(bottleneck)\n",
    "decoder = Dense(64, activation='relu')(decoder)\n",
    "\n",
    "# Output layer\n",
    "output_layer = Dense(n_features, activation='sigmoid')(decoder)  # Use 'sigmoid' or 'relu'\n",
    "\n",
    "# Define the autoencoder model\n",
    "autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# Compile the autoencoder\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you've split your DataFrame into training and validation sets\n",
    "# X_train, X_val = ...\n",
    "df_encoded = df_encoded.astype('float32')\n",
    "df_test_encoded = df_test_encoded.astype('float32')\n",
    "autoencoder.fit(df_encoded, df_encoded,  # The target is the input data itself\n",
    "                epochs=50,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_data=(df_test_encoded, df_test_encoded))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2793/2793 [==============================] - 2s 705us/step\n",
      "2793/2793 [==============================] - 2s 766us/step\n"
     ]
    }
   ],
   "source": [
    "encoder_model = Model(inputs=input_layer, outputs=bottleneck)\n",
    "compressed_data = encoder_model.predict(df_encoded)\n",
    "reconstructed_data = autoencoder.predict(df_encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.9854457e-01, 4.7584024e-01, 1.0000000e+00, ..., 8.6678155e-18,\n",
       "        2.4750526e-04, 1.0569037e-30],\n",
       "       [9.9737459e-01, 5.3435659e-01, 1.0000000e+00, ..., 4.8652789e-20,\n",
       "        3.2783678e-01, 0.0000000e+00],\n",
       "       [4.9697990e-03, 6.0271382e-01, 1.0000000e+00, ..., 1.4124907e-14,\n",
       "        2.7947295e-02, 1.9507396e-31],\n",
       "       ...,\n",
       "       [3.1131611e-03, 5.7107723e-01, 1.0000000e+00, ..., 4.7571149e-14,\n",
       "        8.2414329e-02, 2.6821391e-29],\n",
       "       [9.9998325e-01, 6.2761122e-01, 1.0000000e+00, ..., 3.6573430e-24,\n",
       "        8.0727950e-02, 0.0000000e+00],\n",
       "       [5.3291032e-03, 6.3840473e-01, 1.0000000e+00, ..., 4.3646503e-10,\n",
       "        2.9602233e-01, 4.3871168e-22]], dtype=float32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reconstructed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.manifold import TSNE\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# # Simulating Data\n",
    "# np.random.seed(0)\n",
    "# num_samples = 1000\n",
    "# num_features = 5\n",
    "\n",
    "# # Numerical data\n",
    "# numeric_data = np.random.randn(num_samples, num_features)\n",
    "\n",
    "# # Categorical data (let's say, colors)\n",
    "# colors = ['Red', 'Green', 'Blue']\n",
    "# categorical_data = np.random.choice(colors, size=num_samples)\n",
    "\n",
    "# # Convert categorical data to one-hot encoding\n",
    "# encoder = OneHotEncoder(sparse=False)\n",
    "# categorical_encoded = encoder.fit_transform(categorical_data.reshape(-1, 1))\n",
    "\n",
    "# # Combining numerical and categorical data\n",
    "# combined_data = np.hstack((numeric_data, categorical_encoded))\n",
    "\n",
    "# # Standardize the numerical features\n",
    "# scaler = StandardScaler()\n",
    "# scaled_data = scaler.fit_transform(combined_data)\n",
    "\n",
    "# # Apply PCA\n",
    "# pca = PCA(n_components=0.95)  # Keep 95% of the variance\n",
    "# pca_result = pca.fit_transform(scaled_data)\n",
    "\n",
    "# # Apply t-SNE\n",
    "# tsne = TSNE(n_components=2, perplexity=30, n_iter=300)\n",
    "# tsne_result = tsne.fit_transform(pca_result)\n",
    "\n",
    "# # Plotting the results\n",
    "# sns.set(rc={'figure.figsize':(10,8)})\n",
    "# sns.scatterplot(x=tsne_result[:,0], y=tsne_result[:,1], hue=categorical_data, palette='bright')\n",
    "# plt.title('t-SNE plot of the dataset')\n",
    "# plt.xlabel('t-SNE Axis 1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "\n",
    "input_shape = combined_data.shape[1]  # combined data from previous steps\n",
    "encoding_dim = 32  # example of encoding dimension\n",
    "\n",
    "# This is our input placeholder\n",
    "input_data = Input(shape=(input_shape,))\n",
    "\n",
    "# \"encoded\" is the encoded representation of the input\n",
    "encoded = Dense(encoding_dim, activation='relu')(input_data)\n",
    "\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "decoded = Dense(input_shape, activation='sigmoid')(encoded)\n",
    "\n",
    "# This model maps an input to its reconstruction\n",
    "autoencoder = Model(input_data, decoded)\n",
    "\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "autoencoder.fit(combined_data, combined_data, epochs=50, batch_size=256, shuffle=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building\n",
    "We will build two models: \n",
    "1. A traditional machine learning model using Random Forest.\n",
    "2. A deep learning model using PyTorch.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the first step we will try to use the Random Forest method to get the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the code for Random Forest algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the code for Nerual Network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "# Random Forest\n",
    "X = df_encoded.drop('readmitted', axis=1)\n",
    "Y = df_encoded['readmitted']\n",
    "rf_classifier = HistGradientBoostingClassifier(max_iter=100, random_state=42)\n",
    "rf_classifier.fit(X, Y)\n",
    "\n",
    "# Nerual Network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use testing dataset to predict\n",
    "X_test = df_test_encoded.drop('readmitted', axis=1)\n",
    "Y_test = df_test_encoded['readmitted']\n",
    "\n",
    "Y_pred = rf_classifier.predict(X_test)\n",
    "print(Y_pred)\n",
    "print(Y_test)\n",
    "print(accuracy_score(Y_test,Y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "COMP0169",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
