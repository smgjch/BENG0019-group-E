{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diabetic patients readmission rates preditction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /usr/local/python/3.10.13/lib/python3.10/site-packages (4.66.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: learn2learn in /usr/local/python/3.10.13/lib/python3.10/site-packages (0.2.0)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /home/codespace/.local/lib/python3.10/site-packages (from learn2learn) (1.26.2)\n",
      "Requirement already satisfied: gym>=0.14.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from learn2learn) (0.26.2)\n",
      "Requirement already satisfied: torch>=1.1.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from learn2learn) (2.1.2)\n",
      "Requirement already satisfied: torchvision>=0.3.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from learn2learn) (0.16.2)\n",
      "Requirement already satisfied: scipy in /home/codespace/.local/lib/python3.10/site-packages (from learn2learn) (1.11.4)\n",
      "Requirement already satisfied: requests in /home/codespace/.local/lib/python3.10/site-packages (from learn2learn) (2.31.0)\n",
      "Requirement already satisfied: gsutil in /usr/local/python/3.10.13/lib/python3.10/site-packages (from learn2learn) (5.27)\n",
      "Requirement already satisfied: tqdm in /usr/local/python/3.10.13/lib/python3.10/site-packages (from learn2learn) (4.66.1)\n",
      "Requirement already satisfied: qpth>=0.0.15 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from learn2learn) (0.0.16)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from gym>=0.14.0->learn2learn) (3.0.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from gym>=0.14.0->learn2learn) (0.0.8)\n",
      "Requirement already satisfied: cvxpy>=1.1.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from qpth>=0.0.15->learn2learn) (1.4.1)\n",
      "Requirement already satisfied: filelock in /home/codespace/.local/lib/python3.10/site-packages (from torch>=1.1.0->learn2learn) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /home/codespace/.local/lib/python3.10/site-packages (from torch>=1.1.0->learn2learn) (4.8.0)\n",
      "Requirement already satisfied: sympy in /home/codespace/.local/lib/python3.10/site-packages (from torch>=1.1.0->learn2learn) (1.12)\n",
      "Requirement already satisfied: networkx in /home/codespace/.local/lib/python3.10/site-packages (from torch>=1.1.0->learn2learn) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/codespace/.local/lib/python3.10/site-packages (from torch>=1.1.0->learn2learn) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /home/codespace/.local/lib/python3.10/site-packages (from torch>=1.1.0->learn2learn) (2023.12.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/codespace/.local/lib/python3.10/site-packages (from torch>=1.1.0->learn2learn) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/codespace/.local/lib/python3.10/site-packages (from torch>=1.1.0->learn2learn) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/codespace/.local/lib/python3.10/site-packages (from torch>=1.1.0->learn2learn) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/codespace/.local/lib/python3.10/site-packages (from torch>=1.1.0->learn2learn) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/codespace/.local/lib/python3.10/site-packages (from torch>=1.1.0->learn2learn) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/codespace/.local/lib/python3.10/site-packages (from torch>=1.1.0->learn2learn) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/codespace/.local/lib/python3.10/site-packages (from torch>=1.1.0->learn2learn) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/codespace/.local/lib/python3.10/site-packages (from torch>=1.1.0->learn2learn) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/codespace/.local/lib/python3.10/site-packages (from torch>=1.1.0->learn2learn) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /home/codespace/.local/lib/python3.10/site-packages (from torch>=1.1.0->learn2learn) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/codespace/.local/lib/python3.10/site-packages (from torch>=1.1.0->learn2learn) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /home/codespace/.local/lib/python3.10/site-packages (from torch>=1.1.0->learn2learn) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/codespace/.local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.1.0->learn2learn) (12.3.101)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/codespace/.local/lib/python3.10/site-packages (from torchvision>=0.3.0->learn2learn) (10.1.0)\n",
      "Requirement already satisfied: argcomplete>=1.9.4 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from gsutil->learn2learn) (3.2.1)\n",
      "Requirement already satisfied: crcmod>=1.7 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from gsutil->learn2learn) (1.7)\n",
      "Requirement already satisfied: fasteners>=0.14.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from gsutil->learn2learn) (0.19)\n",
      "Requirement already satisfied: gcs-oauth2-boto-plugin>=3.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from gsutil->learn2learn) (3.0)\n",
      "Requirement already satisfied: google-apitools>=0.5.32 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from gsutil->learn2learn) (0.5.32)\n",
      "Requirement already satisfied: httplib2==0.20.4 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from gsutil->learn2learn) (0.20.4)\n",
      "Requirement already satisfied: google-reauth>=0.1.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from gsutil->learn2learn) (0.1.1)\n",
      "Requirement already satisfied: monotonic>=1.4 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from gsutil->learn2learn) (1.6)\n",
      "Requirement already satisfied: pyOpenSSL>=0.13 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from gsutil->learn2learn) (23.3.0)\n",
      "Requirement already satisfied: retry-decorator>=1.0.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from gsutil->learn2learn) (1.1.1)\n",
      "Requirement already satisfied: six>=1.16.0 in /home/codespace/.local/lib/python3.10/site-packages (from gsutil->learn2learn) (1.16.0)\n",
      "Requirement already satisfied: google-auth>=2.5.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from google-auth[aiohttp]>=2.5.0->gsutil->learn2learn) (2.26.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /home/codespace/.local/lib/python3.10/site-packages (from httplib2==0.20.4->gsutil->learn2learn) (3.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.10/site-packages (from requests->learn2learn) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.10/site-packages (from requests->learn2learn) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from requests->learn2learn) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.10/site-packages (from requests->learn2learn) (2023.11.17)\n",
      "Requirement already satisfied: osqp>=0.6.2 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from cvxpy>=1.1.0->qpth>=0.0.15->learn2learn) (0.6.3)\n",
      "Requirement already satisfied: ecos>=2 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from cvxpy>=1.1.0->qpth>=0.0.15->learn2learn) (2.0.12)\n",
      "Requirement already satisfied: clarabel>=0.5.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from cvxpy>=1.1.0->qpth>=0.0.15->learn2learn) (0.6.0)\n",
      "Requirement already satisfied: scs>=3.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from cvxpy>=1.1.0->qpth>=0.0.15->learn2learn) (3.2.4.post1)\n",
      "Requirement already satisfied: pybind11 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from cvxpy>=1.1.0->qpth>=0.0.15->learn2learn) (2.11.1)\n",
      "Requirement already satisfied: rsa==4.7.2 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from gcs-oauth2-boto-plugin>=3.0->gsutil->learn2learn) (4.7.2)\n",
      "Requirement already satisfied: boto>=2.29.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from gcs-oauth2-boto-plugin>=3.0->gsutil->learn2learn) (2.49.0)\n",
      "Requirement already satisfied: oauth2client>=2.2.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from gcs-oauth2-boto-plugin>=3.0->gsutil->learn2learn) (4.1.3)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from rsa==4.7.2->gcs-oauth2-boto-plugin>=3.0->gsutil->learn2learn) (0.5.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from google-auth>=2.5.0->google-auth[aiohttp]>=2.5.0->gsutil->learn2learn) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from google-auth>=2.5.0->google-auth[aiohttp]>=2.5.0->gsutil->learn2learn) (0.3.0)\n",
      "Requirement already satisfied: aiohttp<4.0.0.dev0,>=3.6.2 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from google-auth[aiohttp]>=2.5.0->gsutil->learn2learn) (3.9.1)\n",
      "Requirement already satisfied: pyu2f in /usr/local/python/3.10.13/lib/python3.10/site-packages (from google-reauth>=0.1.0->gsutil->learn2learn) (0.1.5)\n",
      "Requirement already satisfied: cryptography<42,>=41.0.5 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from pyOpenSSL>=0.13->gsutil->learn2learn) (41.0.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/codespace/.local/lib/python3.10/site-packages (from jinja2->torch>=1.1.0->learn2learn) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/codespace/.local/lib/python3.10/site-packages (from sympy->torch>=1.1.0->learn2learn) (1.3.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/codespace/.local/lib/python3.10/site-packages (from aiohttp<4.0.0.dev0,>=3.6.2->google-auth[aiohttp]>=2.5.0->gsutil->learn2learn) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from aiohttp<4.0.0.dev0,>=3.6.2->google-auth[aiohttp]>=2.5.0->gsutil->learn2learn) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from aiohttp<4.0.0.dev0,>=3.6.2->google-auth[aiohttp]>=2.5.0->gsutil->learn2learn) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from aiohttp<4.0.0.dev0,>=3.6.2->google-auth[aiohttp]>=2.5.0->gsutil->learn2learn) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from aiohttp<4.0.0.dev0,>=3.6.2->google-auth[aiohttp]>=2.5.0->gsutil->learn2learn) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from aiohttp<4.0.0.dev0,>=3.6.2->google-auth[aiohttp]>=2.5.0->gsutil->learn2learn) (4.0.3)\n",
      "Requirement already satisfied: cffi>=1.12 in /home/codespace/.local/lib/python3.10/site-packages (from cryptography<42,>=41.0.5->pyOpenSSL>=0.13->gsutil->learn2learn) (1.16.0)\n",
      "Requirement already satisfied: qdldl in /usr/local/python/3.10.13/lib/python3.10/site-packages (from osqp>=0.6.2->cvxpy>=1.1.0->qpth>=0.0.15->learn2learn) (0.1.7.post0)\n",
      "Requirement already satisfied: pycparser in /home/codespace/.local/lib/python3.10/site-packages (from cffi>=1.12->cryptography<42,>=41.0.5->pyOpenSSL>=0.13->gsutil->learn2learn) (2.21)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install learn2learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy import optimize\n",
    "from sklearn import datasets as skdataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "from learn2learn.algorithms.maml import MAML\n",
    "from learn2learn.data import TaskDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project introduction\n",
    "\n",
    "- Overview: <br>\n",
    "This project is focusing on developing a predictive model to ascertain the likelihood of readmission for diabetes patients.\n",
    "<br>\n",
    "\n",
    "- Target:<br>\n",
    "The main goal of this project is developing a powerful machine learning model which can predict the readmission rate of patient "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading\n",
    "The following cells are used to load training and testing data for our prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"Dataset/diabetic_data_training.csv\")\n",
    "test_data = pd.read_csv(\"Dataset/diabetic_data_test.csv\")\n",
    "mapping_info = pd.read_csv(\"Dataset/IDS_mapping.csv\", header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to handle different data types for plotting\n",
    "# def plot_column(ax, column, df):\n",
    "#     if df[column].dtype == 'object':\n",
    "#         # Check if binary\n",
    "#         if df[column].nunique() == 2:\n",
    "#             # Binary data visualization\n",
    "#             df[column].value_counts().plot(kind='bar', ax=ax)\n",
    "#         else:\n",
    "#             # Categorical data visualization\n",
    "#             df[column].value_counts().plot(kind='pie', autopct='%1.1f%%', ax=ax)\n",
    "#     elif df[column].dtype == 'int64' or df[column].dtype == 'float64':\n",
    "#         # Numeric data visualization\n",
    "#         df[column].plot(kind='hist', bins=20, ax=ax)\n",
    "#     else:\n",
    "#         ax.text(0.5, 0.5, f\"Unhandled data type for column: {column}\", \n",
    "#                 fontsize=12, ha='center')\n",
    "#     ax.set_title(column)\n",
    "\n",
    "# # Creating a 4x4 subplot layout\n",
    "# fig, axes = plt.subplots(nrows=13, ncols=4, figsize=(20, 65))\n",
    "# fig.tight_layout(pad=5.0)\n",
    "\n",
    "# # Iterate through each column and plot\n",
    "# for i, col in enumerate(train_data.columns):\n",
    "#     # Adjust this line to select different subsets of columns  \n",
    "#     plot_column(axes[i//4, i%4], col,train_data)\n",
    "\n",
    "\n",
    "# plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing\n",
    "The following cells are used to preprocess the training and testing data. There are two main goals in our preprocessing data section of the code\n",
    "- Change the string type data in our dataset to integer type data \n",
    "- Apply some applicable method to full up the missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # this part just make a dictionary to mapping the id between data and ID mapping table\n",
    "# mapping_dicts = [{} for i in range(3)]\n",
    "# mapping_title = []\n",
    "# title = -1\n",
    "# for data in mapping_info.itertuples():\n",
    "#     if(pd.isna(data[1])):\n",
    "#         continue\n",
    "#     if(not data[1].isdigit()):\n",
    "#         title += 1\n",
    "#         mapping_title.append(data[1])\n",
    "#     else:\n",
    "#         if(pd.isna(data[2])):\n",
    "#             mapping_dicts[title]['NULL'] = data[1]\n",
    "#             continue\n",
    "#         mapping_dicts[title][data[2]] = data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this part is used to change all string type data to integer type\n",
    "# for the missing value, we will skip and process it at next step\n",
    "df = train_data.copy()\n",
    "\n",
    "\n",
    "df_test = test_data.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# mask = df.isna().sum(axis=1) <= 3\n",
    "# filtered_df = df[mask]\n",
    "# print(filtered_df.any(axis=1).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-Hot Encoding For race:\n",
    "cons: One-hot encoding can lead to a significant increase in the dataset's dimensionality (a problem known as the \"curse of dimensionality\"), especially if the categorical feature has many unique values. This can increase the computational cost and may require more data to achieve good performance.\n",
    "Dems Redct Would be apply, so it doesn't matter\n",
    "pros: Map to a fix number implies an ordinal relationship between the categories which may not exist, but is ideal for non-ordinal categorical data. It's suitable for many machine learning models, especially those that assume no ordinal relationship between categories\n",
    "\n",
    "\n",
    "1. random forest, remove ?\n",
    "2. randomly assign ? to a class by disstribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General Missing value\n",
    "1. multiple imputation To be decide when training if less than 1h 5 epoch\n",
    "2. mean\n",
    "3. fullly remove\n",
    "4. wrong -> fix ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding for age:\n",
    "1. Asumming normal distribution, map to a random age in the range\n",
    "2. Map to mean age in the range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Race Process\n",
    "1. Remove missing since race is proved to be a significant impact to medical result.\n",
    "2. One-hot encode race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def race_filter(df):\n",
    "    \"\"\"\n",
    "    Remove missing value from race\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.dataframe): series containing a colum of the feature matrix.\n",
    "\n",
    "    Return:\n",
    "    Filtered dataframe free from ? for race\n",
    "    \"\"\"\n",
    "    race_mask = (df['race'] != \"?\")\n",
    "    df = df[race_mask]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = race_filter(df)\n",
    "df_encoded = pd.get_dummies(df, columns=[\"race\"], prefix=\"race\",dtype=int)\n",
    "\n",
    "df_test = race_filter(df_test)\n",
    "df_test_encoded = pd.get_dummies(df_test, columns=[\"race\"], prefix=\"race\",dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gender Process\n",
    "1. Remove Unknown/Invalid and missing\n",
    "2. One hot encode Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gender_filter(df):\n",
    "    \"\"\"\n",
    "    Remove missing value from race\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.dataframe): series containing a colum of the feature matrix.\n",
    "\n",
    "    Return:\n",
    "    Filtered dataframe free from ? for race\n",
    "    \"\"\"\n",
    "    gender_mask = (df['gender'] != \"Unknown/Invalid\")\n",
    "    df = df[gender_mask]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded = gender_filter(df_encoded)\n",
    "df_test_encoded = gender_filter(df_test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_mapping = {'Male':0,'Female':1}\n",
    "df_encoded['gender'] = df_encoded['gender'].map(gender_mapping)\n",
    "df_test_encoded['gender'] = df_test_encoded['gender'].map(gender_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encounter_id and patient_nbr Process \n",
    "1. n/a\n",
    "2. Drop since it provide no info to the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded.drop(columns=['encounter_id','patient_nbr'], inplace=True)\n",
    "df_test_encoded.drop(columns=['encounter_id','patient_nbr'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Age Process\n",
    "1. n/a\n",
    "2. Map age from range to mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded['age'] = (df['age'].str.extract(r'(\\d+)-(\\d+)')[0].astype(int)+df['age'].str.extract(r'(\\d+)-(\\d+)')[1].astype(int))//2\n",
    "df_test_encoded['age'] = (df_test['age'].str.extract(r'(\\d+)-(\\d+)')[0].astype(int)+df_test['age'].str.extract(r'(\\d+)-(\\d+)')[1].astype(int))//2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weight Process\n",
    "1. Drop Weight for too many missing values and no information to predict.\n",
    "2. n/a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded.drop(columns=['weight'], inplace=True)\n",
    "df_test_encoded.drop(columns=['weight'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "admission_type_id\n",
    "discharge_disposition_id\n",
    "admission_source_id\n",
    "To be merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Payer Code and Medical Specialty Process\n",
    "1. Both droped since too many missing\n",
    "2. Payer Code not relevant as id, Medical Specialty too many categories and no info to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded.drop(columns=['payer_code'], inplace=True)\n",
    "df_encoded.drop(columns=['medical_specialty'], inplace=True)\n",
    "df_test_encoded.drop(columns=['payer_code'], inplace=True)\n",
    "df_test_encoded.drop(columns=['medical_specialty'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pc = df['payer_code'].astype('category')\n",
    "# unique_categories_pc = df_pc.cat.categories\n",
    "# medical_specialty_to_num = {medical_specialty: i for i, medical_specialty in enumerate(unique_categories_pc)}\n",
    "# max_binary_length = len(bin(len(unique_categories_pc) - 1)) - 2\n",
    "# payer_code_to_binary = {medical_specialty: bin(num)[2:].zfill(max_binary_length) for medical_specialty, num in medical_specialty_to_num.items()}\n",
    "\n",
    "# df_encoded['payer_code'] = df_encoded['payer_code'].map(payer_code_to_binary)\n",
    "\n",
    "# df_test_encoded['payer_code'] = df_test_encoded['payer_code'].map(payer_code_to_binary)\n",
    "# df_ms = df['medical_specialty'].astype('category')\n",
    "# unique_categories_ms = df_ms.cat.categories\n",
    "# medical_specialty_to_num = {medical_specialty: i for i, medical_specialty in enumerate(unique_categories_ms)}\n",
    "# max_binary_length = len(bin(len(unique_categories_ms) - 1)) - 2\n",
    "# medical_specialty_to_binary = {medical_specialty: bin(num)[2:].zfill(max_binary_length) for medical_specialty, num in medical_specialty_to_num.items()}\n",
    "\n",
    "# df_encoded['medical_specialty'] = df_encoded['medical_specialty'].map(medical_specialty_to_binary)\n",
    "\n",
    "# df_test_encoded['medical_specialty'] = df_test_encoded['medical_specialty'].map(medical_specialty_to_binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "num_lab_procedures\n",
    "num_procedures\n",
    "num_medications\n",
    "number_outpatient\n",
    "number_emergency\n",
    "number_inpatient\n",
    "No need to be encode since no missing and integer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "diag_1,diag_2 and diag_3 Process:\n",
    "1. Filte diag_1, diag_2 and diag_3 by number_diagnoses. If the amount of diagnoses in diag_1, diag_2 and diag_3 doesn't match the number_diagnoses, then remove.\n",
    "2. Layer encoding diags according to the ICD-9 Code Category, drop original 3 diag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the encoding function\n",
    "# def encode_element(element):\n",
    "#     encoding_result = [0] * 20  # Default encoding\n",
    "\n",
    "#     if pd.notna(element):\n",
    "#         if element[0].isdigit():\n",
    "#             element = float(element)\n",
    "#             # Encode based on numeric range\n",
    "#             if 1 <= element <= 139:\n",
    "#                 encoding_result[0] = 1\n",
    "#             elif 140 <= element <= 239:\n",
    "#                 encoding_result[1] = 1\n",
    "#             elif 240 <= element <= 279:\n",
    "#                 encoding_result[2] = 1\n",
    "#             elif 280 <= element <= 289:\n",
    "#                 encoding_result[3] = 1\n",
    "#             elif 290 <= element <= 319:\n",
    "#                 encoding_result[4] = 1\n",
    "#             elif 320 <= element <= 389:\n",
    "#                 encoding_result[5] = 1\n",
    "#             elif 390 <= element <= 459:\n",
    "#                 encoding_result[6] = 1\n",
    "#             elif 460 <= element <= 519:\n",
    "#                 encoding_result[7] = 1\n",
    "#             elif 520 <= element <= 579:\n",
    "#                 encoding_result[8] = 1\n",
    "#             elif 580 <= element <= 629:\n",
    "#                 encoding_result[9] = 1\n",
    "#             elif 630 <= element <= 679:\n",
    "#                 encoding_result[10] = 1\n",
    "#             elif 680 <= element <= 709:\n",
    "#                 encoding_result[11] = 1\n",
    "#             elif 710 <= element <= 739:\n",
    "#                 encoding_result[12] = 1\n",
    "#             elif 740 <= element <= 759:\n",
    "#                 encoding_result[13] = 1\n",
    "#             elif 760 <= element <= 779:\n",
    "#                 encoding_result[14] = 1\n",
    "#             elif 780 <= element <= 799:\n",
    "#                 encoding_result[15] = 1\n",
    "#             elif 800 <= element <= 999:\n",
    "#                 encoding_result[16] = 1\n",
    "#             # Add more conditions for other ranges if needed\n",
    "#         elif element[0].isalpha():\n",
    "#             # Encode based on string prefix\n",
    "#             if element.startswith('E'):\n",
    "#                 encoding_result[17] = 1\n",
    "#             elif element.startswith('V'):\n",
    "#                 encoding_result[18] = 1\n",
    "#             elif element.startswith('M'):\n",
    "#                 encoding_result[19] = 1\n",
    "#             # Add more conditions for other prefixes if needed\n",
    "\n",
    "#     return encoding_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = df[\"diag_1\"].loc[0]\n",
    "# a = encode_element(a)\n",
    "# a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diag_missing_value_filter(df):\n",
    "\n",
    "    df['number_diagnoses'] = pd.to_numeric(df['number_diagnoses'], errors='coerce')\n",
    "    mask = ((df['number_diagnoses'] <= 3) & (\n",
    "            (3 - df[['diag_1', 'diag_2', 'diag_3']].apply(lambda x: x == \"?\").sum(axis=1)) >= df['number_diagnoses'])) | (\n",
    "    (df['number_diagnoses'] > 3) & (df[['diag_1', 'diag_2', 'diag_3']].apply(lambda x: x == \"?\").sum(axis=1)) == 0) \n",
    "    df_filtered = df[mask].copy()\n",
    "\n",
    "    return df_filtered\n",
    "\n",
    "def diag_layer_encoding(df):\n",
    "\n",
    "    # Define the encoding function\n",
    "    def encode_element(element):\n",
    "        encoding_result = [0] * 20  # Default encoding\n",
    "\n",
    "        if pd.notna(element):\n",
    "            if element[0].isdigit():\n",
    "                element = float(element)\n",
    "                # Encode based on numeric range\n",
    "                if 1 <= element <= 139:\n",
    "                    encoding_result[0] = 1\n",
    "                elif 140 <= element <= 239:\n",
    "                    encoding_result[1] = 1\n",
    "                elif 240 <= element <= 279:\n",
    "                    encoding_result[2] = 1\n",
    "                elif 280 <= element <= 289:\n",
    "                    encoding_result[3] = 1\n",
    "                elif 290 <= element <= 319:\n",
    "                    encoding_result[4] = 1\n",
    "                elif 320 <= element <= 389:\n",
    "                    encoding_result[5] = 1\n",
    "                elif 390 <= element <= 459:\n",
    "                    encoding_result[6] = 1\n",
    "                elif 460 <= element <= 519:\n",
    "                    encoding_result[7] = 1\n",
    "                elif 520 <= element <= 579:\n",
    "                    encoding_result[8] = 1\n",
    "                elif 580 <= element <= 629:\n",
    "                    encoding_result[9] = 1\n",
    "                elif 630 <= element <= 679:\n",
    "                    encoding_result[10] = 1\n",
    "                elif 680 <= element <= 709:\n",
    "                    encoding_result[11] = 1\n",
    "                elif 710 <= element <= 739:\n",
    "                    encoding_result[12] = 1\n",
    "                elif 740 <= element <= 759:\n",
    "                    encoding_result[13] = 1\n",
    "                elif 760 <= element <= 779:\n",
    "                    encoding_result[14] = 1\n",
    "                elif 780 <= element <= 799:\n",
    "                    encoding_result[15] = 1\n",
    "                elif 800 <= element <= 999:\n",
    "                    encoding_result[16] = 1\n",
    "                # Add more conditions for other ranges if needed\n",
    "            elif element[0].isalpha():\n",
    "                # Encode based on string prefix\n",
    "                if element.startswith('E'):\n",
    "                    encoding_result[17] = 1\n",
    "                elif element.startswith('V'):\n",
    "                    encoding_result[18] = 1\n",
    "                elif element.startswith('M'):\n",
    "                    encoding_result[19] = 1\n",
    "                # Add more conditions for other prefixes if needed\n",
    "\n",
    "        return encoding_result\n",
    "\n",
    "    for i in range(1,4):\n",
    "        encoded_columns = df[\"diag_\"+str(i)].apply(encode_element)\n",
    "\n",
    "    # Create new columns with \"diag_1\" as a prefix\n",
    "        for j in range(20):\n",
    "            new_column_name = \"diag_\" + str(i)+\"_\"+ str(j+1)\n",
    "            df[new_column_name] = encoded_columns.apply(lambda x: x[j])\n",
    "\n",
    "    return df\n",
    "# def diag_layer_encoding(element):\n",
    "    if isinstance(element, (int,float)):\n",
    "        # Encode based on numeric range\n",
    "        if 1 <= element <= 139:\n",
    "            return [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "        elif 140 <= element <= 239:\n",
    "            return [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "        elif 240 <= element <= 279:\n",
    "            return [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "        elif 280 <= element <= 289:\n",
    "            return [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "        elif 290 <= element <= 319:\n",
    "            return [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "        elif 320 <= element <= 389:\n",
    "            return [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "        elif 390 <= element <= 459:\n",
    "            return [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "        elif 460 <= element <= 519:\n",
    "            return [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "        elif 520 <= element <= 579:\n",
    "            return [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "        elif 580 <= element <= 629:\n",
    "            return [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "        elif 630 <= element <= 679:\n",
    "            return [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "        elif 680 <= element <= 709:\n",
    "            return [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "        elif 710 <= element <= 739:\n",
    "            return [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "        elif 740 <= element <= 759:\n",
    "            return [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
    "        elif 760 <= element <= 779:\n",
    "            return [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
    "        elif 780 <= element <= 799:\n",
    "            return [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
    "        elif 800 <= element <= 999:\n",
    "            return [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
    "        # Add more conditions for other ranges if needed\n",
    "\n",
    "    elif isinstance(element, str):\n",
    "        # Encode based on string prefix\n",
    "        if element.startswith('E'):\n",
    "            return [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
    "        elif element.startswith('V'):\n",
    "            return [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
    "        elif element.startswith('M'):\n",
    "            return [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    "        # Add more conditions for other prefixes if needed\n",
    "\n",
    "    # Default case (not a number or string with known prefix)\n",
    "    return [0] * 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded = diag_missing_value_filter(df_encoded)\n",
    "df_encoded = diag_layer_encoding(df_encoded)\n",
    "df_test_encoded = diag_missing_value_filter(df_test_encoded)\n",
    "df_test_encoded = diag_layer_encoding(df_test_encoded)\n",
    "\n",
    "for i in range(1,4):\n",
    "        df_encoded.drop(columns=[f'diag_{i}'], inplace=True)\n",
    "        df_test_encoded.drop(columns=[f'diag_{i}'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "number_diagnoses pass since no missing and integer\n",
    "max_glu_serum and A1Cresult Process:\n",
    "1.n/a\n",
    "2.index map to 0-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_glu_serum_mapping = {'>200': 2, '>300': 3, 'normal': 1}\n",
    "A1Cresult_mapping = {'>8':3,'>7':2,'normal':1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded['max_glu_serum'] = df_encoded['max_glu_serum'].map(max_glu_serum_mapping).fillna(0)\n",
    "df_test_encoded['max_glu_serum'] = df_test_encoded['max_glu_serum'].map(max_glu_serum_mapping).fillna(0)\n",
    "\n",
    "df_encoded[\"A1Cresult\"] = df_encoded['A1Cresult'].map(A1Cresult_mapping).fillna(0)\n",
    "df_test_encoded[\"A1Cresult\"] = df_test_encoded['A1Cresult'].map(A1Cresult_mapping).fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "metformin\n",
    "repaglinide\n",
    "nateglinide\n",
    "chlorpropamide\n",
    "glimepiride\n",
    "acetohexamide\n",
    "glipizide\n",
    "glyburide\n",
    "tolbutamide\n",
    "pioglitazone\n",
    "rosiglitazone\n",
    "acarbose\n",
    "miglitol\n",
    "troglitazone\n",
    "tolazamide\n",
    "examide\n",
    "citoglipton\n",
    "insulin\n",
    "glyburide-metformin\n",
    "glipizide-metformin\n",
    "glimepiride-pioglitazone\n",
    "metformin-rosiglitazone\n",
    "metformin-pioglitazone\n",
    "Process:\n",
    "1. drop 'examide', 'citoglipton','glimepiride-pioglitazone' and 'metformin-rosiglitazone' since single value observed\n",
    "2. index map to 0-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded.drop(columns=['examide', 'citoglipton','glimepiride-pioglitazone','metformin-rosiglitazone'],axis=1,inplace=True)\n",
    "df_test_encoded.drop(columns=['examide', 'citoglipton','glimepiride-pioglitazone','metformin-rosiglitazone'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "medics = ['metformin','repaglinide', 'nateglinide', 'chlorpropamide', 'glimepiride',\n",
    "       'acetohexamide', 'glipizide', 'glyburide', 'tolbutamide',\n",
    "       'pioglitazone', 'rosiglitazone', 'acarbose', 'miglitol', 'troglitazone',\n",
    "       'tolazamide', 'insulin', 'glyburide-metformin', 'glipizide-metformin',\n",
    "        'metformin-pioglitazone']\n",
    "for i in medics:\n",
    "    df_encoded.loc[df_encoded[i] == 'Up', [i]] = 3  \n",
    "    df_encoded.loc[df_encoded[i] == 'Down', [i]] = 1 \n",
    "    df_encoded.loc[df_encoded[i] == 'Steady', [i]] = 2 \n",
    "    df_encoded.loc[df_encoded[i] == 'No', [i]] = 0\n",
    "    df_test_encoded.loc[df_test_encoded[i] == 'Up', [i]] = 3  \n",
    "    df_test_encoded.loc[df_test_encoded[i] == 'Down', [i]] = 1 \n",
    "    df_test_encoded.loc[df_test_encoded[i] == 'Steady', [i]] = 2 \n",
    "    df_test_encoded.loc[df_test_encoded[i] == 'No', [i]] = 0 \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "change and diabetesMed Process\n",
    "1. n/a\n",
    "2. binary map to 1/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_mapping = {'No':0,'Ch':1}\n",
    "diabetesMed_mapping = {'No':0,'Yes':1}\n",
    "\n",
    "df_encoded['change'] = df_encoded['change'].map(change_mapping)\n",
    "df_encoded['diabetesMed'] = df_encoded['diabetesMed'].map(diabetesMed_mapping)\n",
    "\n",
    "df_test_encoded['change'] = df_test_encoded['change'].map(change_mapping)\n",
    "df_test_encoded['diabetesMed'] = df_test_encoded['diabetesMed'].map(diabetesMed_mapping)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "readmitted Process\n",
    "1.n/a\n",
    "2. index mapping, No as 0, >30 as 1 and <30 as most significant as 2 and drop readmitted for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "readmitted_mapping = {'NO':0,'<30':2,'>30':1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_encoded['readmitted'] = df_encoded['readmitted'].map(readmitted_mapping)\n",
    "df_test_encoded['readmitted'] = df_test_encoded['readmitted'].map(readmitted_mapping)\n",
    "\n",
    "y_test_readmitted = df_encoded[\"readmitted\"]\n",
    "y_readmitted = df_test_encoded[\"readmitted\"]\n",
    "\n",
    "df_encoded.drop(columns=[\"readmitted\"],axis=1,inplace=True)\n",
    "df_test_encoded.drop(columns=[\"readmitted\"],axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encode payer_code, medical_specialty\n",
    "1. Find correlation internally with other feature in group of non-missing value\n",
    "2. Use identified feature predict payer-code, medical_specialty\n",
    "3. Prediction algorithm to be decide, could be KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this part will reduce the dimension our training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dems Redct\n",
    "1. PCA/PPCA\n",
    "2. LDA/QDA\n",
    "3. following to T-SNE\n",
    "3. Autoencoders\n",
    "4. Unsupervised Algorithmn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sns' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 39\u001b[0m\n\u001b[1;32m     36\u001b[0m tsne_result \u001b[38;5;241m=\u001b[39m tsne\u001b[38;5;241m.\u001b[39mfit_transform(pca_result)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Plotting the results\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m \u001b[43msns\u001b[49m\u001b[38;5;241m.\u001b[39mset(rc\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfigure.figsize\u001b[39m\u001b[38;5;124m'\u001b[39m:(\u001b[38;5;241m10\u001b[39m,\u001b[38;5;241m8\u001b[39m)})\n\u001b[1;32m     40\u001b[0m sns\u001b[38;5;241m.\u001b[39mscatterplot(x\u001b[38;5;241m=\u001b[39mtsne_result[:,\u001b[38;5;241m0\u001b[39m], y\u001b[38;5;241m=\u001b[39mtsne_result[:,\u001b[38;5;241m1\u001b[39m], hue\u001b[38;5;241m=\u001b[39mcategorical_data, palette\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbright\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     41\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt-SNE plot of the dataset\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sns' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Simulating Data\n",
    "np.random.seed(0)\n",
    "num_samples = 1000\n",
    "num_features = 5\n",
    "\n",
    "# Numerical data\n",
    "numeric_data = np.random.randn(num_samples, num_features)\n",
    "\n",
    "# Categorical data (let's say, colors)\n",
    "colors = ['Red', 'Green', 'Blue']\n",
    "categorical_data = np.random.choice(colors, size=num_samples)\n",
    "\n",
    "# Convert categorical data to one-hot encoding\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "categorical_encoded = encoder.fit_transform(categorical_data.reshape(-1, 1))\n",
    "\n",
    "# Combining numerical and categorical data\n",
    "combined_data = np.hstack((numeric_data, categorical_encoded))\n",
    "\n",
    "# Standardize the numerical features\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(combined_data)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=0.95)  # Keep 95% of the variance\n",
    "pca_result = pca.fit_transform(scaled_data)\n",
    "\n",
    "# Apply t-SNE\n",
    "tsne = TSNE(n_components=2, perplexity=30, n_iter=300)\n",
    "tsne_result = tsne.fit_transform(pca_result)\n",
    "\n",
    "# Plotting the results\n",
    "sns.set(rc={'figure.figsize':(10,8)})\n",
    "sns.scatterplot(x=tsne_result[:,0], y=tsne_result[:,1], hue=categorical_data, palette='bright')\n",
    "plt.title('t-SNE plot of the dataset')\n",
    "plt.xlabel('t-SNE Axis 1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "\n",
    "input_shape = combined_data.shape[1]  # combined data from previous steps\n",
    "encoding_dim = 32  # example of encoding dimension\n",
    "\n",
    "# This is our input placeholder\n",
    "input_data = Input(shape=(input_shape,))\n",
    "\n",
    "# \"encoded\" is the encoded representation of the input\n",
    "encoded = Dense(encoding_dim, activation='relu')(input_data)\n",
    "\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "decoded = Dense(input_shape, activation='sigmoid')(encoded)\n",
    "\n",
    "# This model maps an input to its reconstruction\n",
    "autoencoder = Model(input_data, decoded)\n",
    "\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "autoencoder.fit(combined_data, combined_data, epochs=50, batch_size=256, shuffle=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building\n",
    "We will build two models: \n",
    "1. A traditional machine learning model using Random Forest.\n",
    "2. A deep learning model using PyTorch.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the first step we will try to use the Random Forest method to get the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the code for Random Forest algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the code for Nerual Network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "# Random Forest\n",
    "X = df_encoded.drop('readmitted', axis=1)\n",
    "Y = df_encoded['readmitted']\n",
    "rf_classifier = HistGradientBoostingClassifier(max_iter=100, random_state=42)\n",
    "rf_classifier.fit(X, Y)\n",
    "\n",
    "# Nerual Network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use testing dataset to predict\n",
    "X_test = df_test_encoded.drop('readmitted', axis=1)\n",
    "Y_test = df_test_encoded['readmitted']\n",
    "\n",
    "Y_pred = rf_classifier.predict(X_test)\n",
    "print(Y_pred)\n",
    "print(Y_test)\n",
    "print(accuracy_score(Y_test,Y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "COMP0169",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
